# Agent

Run inference queries with tools.

```bash
npm install @agent-smith/agent
```

Supported backends:

- [Llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Koboldcpp](https://github.com/LostRuins/koboldcpp)
- [Ollama](https://github.com/ollama/ollama)
- Any server that supports an Openai compatible api

## Initialize an agent

### Define a backend

For a local Llama.cpp:

```js
import { Lm } from "@locallm/api";

const lm = new Lm({
    providerType: "llamacpp",
    serverUrl: "http://localhost:8080",
    onToken: (t) => process.stdout.write(t),
});
```

For a remote Openrouter backend:

```js
import { Lm } from "@locallm/api";

const lm = new Lm({
    providerType: "openai",
    serverUrl: "https://openrouter.ai/api/v1",
    apiKey: process.env.OPENROUTER_API_KEY,
    onToken: (t) => process.stdout.write(t),
});
```

### Initialize an agent

```js
import { Agent } from "@agent-smith/agent";

const agent = new Agent(lm);
```

### Run an inference query

```js
const _prompt = "List the planets of the solar system";
const inferenceParams = {
    stream: true,
    model: "qwen/qwen3-30b-a3b:free",
    temperature: 0.6,
    top_k: 40,
    top_p: 0.95,
    min_p: 0,
    max_tokens: 4096,
};
const options = {
    //debug: true,
    verbose: true,
    //tools: [get_current_traffic, get_current_weather]
};
const result = await agent.run(_prompt, inferenceParams, options);
console.log(result);
```

<a href="javascript:openLink('/libraries/agent/tools')">Next: tools</a>