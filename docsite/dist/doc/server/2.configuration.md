# Configuration

First generate a config file:

```bash
./server -conf
```

This will create a `./server.config.yml` file with a random api key and allowed
cors origins:

```yaml
api_key: "30f224ea6b45bf61356b8eb0bd84d2011f6f85dec6d49716c686cff66510efba"
origins:
  - http://localhost:5173
  - http://localhost:5143
```

## Features

Configure the directories where the yaml tasks definitions are stored:

```yaml
features:
  - /home/me/features/featureset1
  - /home/me/features/featureset2
```

## Models

Some tasks can specify different models sizes. It is possible to define
a model size to be used per task, instead of the default model of the task.
For a `translate` task:

```yaml
models:
  translate: small
```

Available model sizes:

- `xsmall`: up to 4B: suitable for cpu only or very low vram
- `small`: 7B / 9B
- `medium`: 14B / 22B
- `large`: 32B
- `xlarge`: 70B
- `xxlarge`: more than 70B

## Backends

### Ollama

The default backend is a local Ollama instance. Configure the endpoint and
api key using environement variables: check the Ollama doc.

### OpenAi api

To use any OpenAi compatible api configure it:

```yaml
oai_api:
  base_url: http://localhost:11434/v1 # or any api service url
  api_key: my_api_key_here
```

Configure the tasks that will use the api in the `models`Â section:

```yaml
models:
  translate: api
```

<a href="javascript:openLink('/server/tasks')">Next: tasks</a>

