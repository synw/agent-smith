# Models

Collections of models can be specified to use for inference tasks. These can
be specified in a `models` directory of a plugin.

Example with the `@agent-smith/feat-models` plugin. It provide several models
collections:

- <kbd>Inference</kbd>: instruct models of various sizes
- <kbd>Thinking</kbd>: a collection of thinking models
- <kbd>Mini</kbd>: small models of 4b or less
- <kbd>Vision</kbd>: vision models

## Use a models set

To use a model set in a task, declare it. For a basic infence task:

```yaml
description: Run a raw inference query
prompt: |-
    {prompt}
modelset: 
  name: inference
  default: 4b
```

This will use the inference models set, with the default model beeing the 4b. To run
this task (let's call it inference) with a different model size from the set:

```bash
lm inference "my prompt here" s=12b
```

## Define a model set

Format to define a list of models:

```yaml
ctx: number
max_tokens: number
2b: # any string
  name: string # a model name
  template: string # a template name
# ...
```

Example of a model set in yaml:

```yaml
ctx: 32768
max_tokens: 16384
2b:
  name: granite3.2:2b-instruct-q8_0
  template: granite
3b:
  name: qwen2.5:3b-instruct-q8_0
  template: chatml
3.8b:
  name: phi4-mini:3.8b-q8_0
  template: phi4
4b:
  name: nemotron-mini:4b-instruct-q8_0
  template: nemotron
# ...
```

Required keys:

- `ctx`: the default context length to use
- `max_tokens`: the default limit of token to emit

For models the `name` and `template` fields are required. Check the full 
<a href="https://github.com/synw/modprompt/blob/main/codegen/db.yml">list of available templates</a>

## Models parameters override

The default ctx and max_tokens is used, but some models may required different settings. It
is possible to override on a per model basis. 

### Context

To use different context parameters than the defaults for a model:

```yaml
ctx: 32768
max_tokens: 16384
# ...
32b:
  name: qwen2.5:32b
  template: chatml
  ctx: 16384
  inferParams:
    max_tokens: 8192
```

In this example we are overriding the default context params for the Qwen 2.5 32b to lower
the context to fit in 24G vram. Smaller models will use the default 32k context window.

### Inference params

Some models might require some special inference params to run well. Example:

```yaml
ctx: 32768
max_tokens: 16384
# ...
32b:
    name: qwq:32b
    template: chatml
    inferParams:
        top_k: 20
        top_p: 0.95
        min_p: 0
        repeat_penalty: 1
```

We use those inference parameters for QwQ, overriding those task's inference parameters

### Templating

Some models might require a special system prompt or me might want to start the
assistant's response for a model. Example:

```yaml
ctx: 32768
max_tokens: 16384
# ...
8b:
    name: hf.co/bartowski/NousResearch_DeepHermes-3-Llama-3-8B-Preview-GGUF:Q6_K_L
    template: llama3
    system: |-
        You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.
    assistant: "<think>"
  
```

This Deephermes model requires this special system prompt to work well. We also start the
assistant's response with a thinking tag.

<a href="javascript:openLink('/terminal_client/actions')">Next: Actions</a>