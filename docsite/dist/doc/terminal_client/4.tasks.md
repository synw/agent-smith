# Tasks

A task is a prompt for a language model, defined in yaml

## List tasks

The `tasks` command, see the example above

## Read a task

To read a task details:

```bash
lm task mytask
```

It will display the task in json

## Run a task

```bash
lm mytaskname "param 1"
```

For an `infer.yml` task:

```yml
name: infer
description: A test inference task
ctx: 8192
template: 
    name: mistral
prompt: |-
    {prompt}
model:
    name: mistral:instruct
    ctx: 4092
inferParams:
    min_p: 0.05
    temperature: 0.2
```

All the tasks require a prompt parameter. To run this task with a custom prompt:

```bash
lm infer "List the planets in the solar system"
```

The top `ctx` parameter is the default context lenght for the task

## Inference parameters

The following parameters can be used to control the inference process. For a full list of available parameters, 
see the <a href="javascript:openLink('/libraries/lm_task/specification')">LmTask specification</a>:

- `--model (-m)`: Specify the model name (e.g., `phi3.5:latest`)
- `--ctx (-x)`: Set context window size
- `--template (--tpl)`: Define the template format (e.g., `chatml`, `deepseek2`)
- `--max_tokens (--mt)`: Limit the number of generated tokens
- `--top_k (-k)`: Filter results to top K candidates
- `--top_p (-p)`: Use cumulative probability thresholding
- `--min_p (--mp)`: Set minimum probability for token consideration
- `--temperature (-t)`: Control randomness in sampling (0=deterministic, 1=maximum randomness)
- `--repeat_penalty (-r)`: Adjust penalty for repeated tokens
- `--verbose (-v)`: Enable verbose output
- `--debug (-d)`: Show debug information
- `--tokens`: Toggle token visualization mode
- `--chat (-c)`: Enable chat mode for interactive tasks

### Examples

#### Model override

To use a different model than the one specified in the task use
the `-m` flag:

```bash
lm infer "List the planets in the solar system" -m qwen3:4b
```

The program will try to guess the template to use, based on the name of
the model. It works for well known models. To specify a template:

```bash
lm infer "prompt here" -m some_model.gguf --template chatml
```

#### Cumulative Sampling Parameters

1. **Top-k and Top-p with Temperature:**

   ```bash
   lm infer "Generate a creative story about a futuristic city" --top_k 50 --top_p 0.95 --temperature 0.7
   ```

   This command uses top-k filtering to consider the top 50 tokens, top-p (nucleus sampling) with a cumulative probability threshold of 0.95, and a temperature of 0.7 to introduce some randomness.

2. **Min Probability and Repeat Penalty:**

   ```bash
   lm infer "Explain quantum mechanics in simple terms" --min_p 0.01 --repeat_penalty 1.2
   ```

   This command sets a minimum probability threshold for token consideration at 0.01 and applies a repeat penalty of 1.2 to discourage the model from repeating tokens.

3. **Combining Multiple Parameters:**

   ```bash
   lm infer "Describe the process of photosynthesis" --top_k 40 --top_p 0.9 --temperature 0.5 --min_p 0.02 --repeat_penalty 1.1
   ```

   This command combines top-k filtering, nucleus sampling, temperature control, minimum probability threshold, and repeat penalty to fine-tune the inference process.

#### Debug, Verbose, Show Tokens Modes

1. **Verbose Mode:**

   ```bash
   lm infer "What is the capital of France?" --verbose
   ```

   This shows the thinking process for thinking models, and displays the inference parameters used.

2. **Debug Mode:**

   ```bash
   lm infer "Translate 'Hello world' to Spanish" --debug
   ```

   This command shows debug information, which can be useful for troubleshooting and understanding the internal workings of the model.

3. **Show Tokens Mode:**

   ```bash
   lm infer "Generate a poem about nature" --tokens
   ```

   This command toggles token visualization mode, displaying the tokens generated by the model during inference.

#### Chat Mode

1. **Chat Mode Example:**

   ```bash
   lm infer "Start a conversation about AI ethics" --chat
   ```

   This command enables chat mode, allowing for interactive conversations with the language model.

## Input mode

An inference task requires a prompt. Example `infer.yml` task:

```yaml
name: explain
description: Explain code
prompt: |-
      I have this code:

      ```
      {prompt}
      ```

      Explain what the code does in details
template: 
    name: deepseek
    system: You are an AI programmer assistant
model:
    name: deepseek-coder:6.7b
    ctx: 8192
inferParams:
    min_p: 0.05
    temperature: 0.2
```

### Command line input

By default the input is taken from a command line parameter:

```bash
lm infer "some prompt"
```

### Clipboard input

To use input from the clipboard copy some code and run the command
with the `--ic` option:

```bash
lm explain --ic
```

### Promptfile input

The input can be taken from a prompt file. First declare the prompt
file path in your config and update it: `features/config.yml`:

```yml
promptfile: /home/me/lm/features/src/prompt.txt
# ...
```

You can now edit the file and use it as prompt input:


```bash
lm explain --if
```

## Output mode

### Markdown output

By default the output of a task will print to the terminal in
markdown mode, rendering a pretty print of markdown text and code

### Clipboad output

To copy the output of a task into the clipoard use the `--oc` option. Note:
the input and output options can be combined. Example of a clipoard input
and output:

```bash
lm some_code_task --ic --oc
```

## System prompt

Use the template section to add a system prompt:

```yaml
name: tsdoc
description: Create a Jsdoc docstring for a block of code
ctx: 8192
template: 
    name: chatml
    system: |-
        You are an AI programming assistant. Your task is to create detailled and helpful
        documentation.
```

## Assistant block and stop tokens

To make the assistant response start use the template block,. It is useful for 
better autocomplete and formating in some cases:

```yaml
template: 
    name: deepseek2
    system: |-
        You are an AI programming assistant. Your task is to create detailled and helpful
        documentation.
    stop:
        - "```"
    assistant: |-
        Here is the docstring:

        ```ts
```

## Shots

It is possible to use few shots prompting in a task definition. Example with
a create Python docstring task:

```yaml
name: py_docstring
description: Create a Google style docstring for Python code
ctx: 8192
prompt: |-
    in Python create a detailed and helpful Google style docstring for this code:

    ```python
    {prompt}
    ```

    Always provide a short example in the docstring. The code is formatted with Black. 
    Important: output only the docstring
template: 
    name: deepseek2
model:
    name: DeepSeek-Coder-V2-Lite-Instruct-Q8_0
    ctx: 8192
inferParams:
    temperature: 0
shots:
    - user: |-
          def add(a: float, b: float = 2.5) -> float:
            if a < 0:
                raise ValueError("Provide a positive number for a")
            try:
                return a + b
            except Exception as e:
                raise (e)
          
      assistant: |-
          """
          Sums two float numbers, but ensures the first number is non-negative. If the
          second number is not provided, it defaults to 2.5.

          Args:
              a (float): The first number to be added. Must be a non-negative float.
              b (float, optional): The second number to be added. Defaults to 2.5. Can be
                  any float.

          Returns:
              float: The sum of a and b.

          Raises:
              ValueError: If \`a\` is negative.
              Exception: Any unexpected error that might occur during addition.

          Example:
              >>> add(2.5)
              5.0
              >>> add(2.5, 3.5)
              6.0
              >>> add(-1.0)
              ValueError: Provide a positive number for a
          """
```

## Variables

A task always take a `prompt` input. It is possible to add custom variables in
the prompt. 

### Required variables

```yaml
name: translate
description: A translation task
ctx: 4096
prompt: |-
    Translate this text to {lang}:

    ```
    {prompt}
    ```
modelpack:
  default: qwen4b
inferParams:
  min_p: 0.05
  temperature: 0.2
  max_tokens: 2048
variables:
  required:
    - lang
```

The `msg` variable is required. To run the task with a lang value;

```bash
lm translate "some sentence" --lang spanish
```

Note: everytime you change variables in a task run `lm update` to register the changes

### Optional variables

```yaml
name: a_task
ctx: 8192
prompt: |-
      {prompt}{instructions}
variables:
    optional:
        - instructions
```

Here the `instructions` variable is optional:

```bash
lm a_task --ic --instructions "Adopt a very formal tone"
```

In this example the prompt is the content of the clipboard (see the `--ic` option doc)

<a href="javascript:openLink('/terminal_client/models')">Next: Models</a>