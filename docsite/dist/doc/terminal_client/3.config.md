# Config

The custom features (tasks, workflows, actions and commands) are located either
in a local folder or in a js plugin.

You can declare your custom features in a config file.
Create a `features/config.yml` file:

```yml
promptfile: /home/me/lm/features/prompt.txt
features:
  - /home/me/lm/features/dist
plugins:
  - "@agent-smith/feat-git"
```

All are optional. The promptfile is a file that can be used for input for
inference tasks. See the <a href="javascript:openLink('/terminal_client/tasks')">tasks</a>
section for more info about input modes.

Update your client config using this file by running this command:

```bash
lm conf ~/lm/features/config.yml
```

## Backends

Agent Smith support different kinds of backends:

- [Llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Koboldcpp](https://github.com/LostRuins/koboldcpp)
- [Ollama](https://github.com/ollama/ollama)
- Any server that supports an Openai compatible api

To configure your backends in `config.yml`:

```yaml
backends:
  default: "llamacpp"
  local: 
    - "llamacpp"
    - "koboldcpp"
    - "ollama"
  openrouter:
    type: "openai"
    url: "https://openrouter.ai/api/v1"
    apiKey: "$OPENROUTER_API_KEY"
  llamacpp_oai:
    type: "openai"
    url: "http://localhost:8080/v1"
```

The backend listed in `local`Â are default local instances of these servers. The
other ones are custom configs. By default the cli will use the backend configured
in `default`.

List available backends:

```bash
lm backends
```

To change the default backend in the command line:

```bash
lm backend openrouter
```

To use a given backend once for a given task:

```bash
lm mytask "my prompt" -b koboldcpp
```

## Local features

The features are declared in a folder (here `/home/me/lm/features/dist`) using
these subfolders:

- actions
- tasks
- workflows
- cmds

All are optional.

Every new feature will be detected when running `lm update` and then be available

<a href="javascript:openLink('/terminal_client/tasks')">Next: Tasks</a>