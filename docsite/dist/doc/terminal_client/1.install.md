# Install

Install the Agent Smith terminal client globally:

```bash
npm i -g @agent-smith/cli
```

A global `lm` command is available once installed.

## Quickstart

Install the inference plugin:

```bash
npm i -g @agent-smith/feat-inference
```

Create a `config.yml` file:

```yml
plugins:
  - "@agent-smith/feat-inference"
backends:
  default: "llamacpp"
  local: # locally supported backends
    - "llamacpp"
    - "koboldcpp"
    - "ollama"
  openrouter:
    type: "openai"
    url: "https://openrouter.ai/api/v1"
    apiKey: "$OPENROUTER_API_KEY"
  llamacpp_oai: #Â llama.cpp openai compatible api
    type: "openai"
    url: "http://localhost:8080/v1"
```

Run the conf command to initialize:

```bash
lm conf ~/path/to/config.yml
```

### Inference

Run an inference query with the <kbd>q</kbd> command:

```bash
lm q list the planets of the solar system
```

The default model is qwen4b. To use another model:

```bash
lm q list the planets of the solar system -m gemma4b
```

### Tasks

List the available tasks:

```bash
lm tasks
```

Show a task:

```bash
lm task infer
```

<a href="javascript:openLink('/terminal_client/overview')">Next: Overview</a>