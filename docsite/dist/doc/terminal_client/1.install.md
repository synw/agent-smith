# Install

Install the Agent Smith terminal client globally:

```bash
npm i -g @agent-smith/cli
```

A global `lm` command is available once installed.

## Quickstart

Install the inference and the vision plugin:

```bash
npm i -g @agent-smith/feat-inference @agent-smith/feat-vision
```

Create a `config.yml` file with this content:

```yml
plugins:
  - "@agent-smith/feat-inference"
  - "@agent-smith/feat-vision"
  - "@agent-smith/feat-models"
```

Run the conf command to initialize:

```bash
lm conf ~/path/to/config.yml
```

### Inference

Run an inference query with the <kbd>q</kbd> command, using an Ollama model that you already have:

```bash
lm q list the planets of the solar system m="llama3.1:latest"
```

### Vision

Run a vision query: pull `minicpm-v:8b-2.6-q8_0` from Ollama and run:

```bash
lm vision imgs/img1.jpeg imgs/img2.jpeg "Compare the images"
```

### Tasks

List the available tasks:

```bash
lm tasks
```

Show a task:

```bash
lm task think
```

### Models

Show available models:

```bash
lm models
```

Search for a model:

```bash
lm model qwen
```

<a href="javascript:openLink('/terminal_client/overview')">Next: Overview</a>