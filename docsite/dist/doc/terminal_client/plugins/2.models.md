# Models

![pub package](https://img.shields.io/npm/v/@agent-smith/feat-models)

Collections of models to use for inference tasks

## Install

```bash
npm i @agent-smith/feat-models
```

Add the plugin to your config file and run the `conf`Â command to update the cli

## Usage

To use a models set in a task:

```yaml
description: Run an inference query with a thinking model
prompt: |-
    {prompt}
modelset: 
  name: thinking
  default: qwq
```

## Collections

Available models collections:

- <kbd>code</kbd>: code-specific models
- <kbd>inference</kbd>: instruct models of various sizes
- <kbd>thinking</kbd>: a collection of thinking models
- <kbd>mini</kbd>: small models of 4b or less
- <kbd>multilingual</kbd>: multilingual models
- <kbd>tools</kbd>: tools-specific models
- <kbd>vision</kbd>: vision models

### Code

Available models:

- <kbd>qwencoder7b</kbd>: qwen2.5-coder:latest
- <kbd>qwencoder14b</kbd>: qwen2.5-coder:14b
- <kbd>qwencoder32b</kbd>: qwen2.5-coder:32b ( ctx: 16384, max_tokens: 8192 )
- <kbd>qwencoder7bq8</kbd>: qwen2.5-coder:7b-instruct-q8_0
- <kbd>qwencoder14bq8</kbd>: qwen2.5-coder:14b-instruct-q8_0
- <kbd>qwencoder32bq8</kbd>: qwen2.5-coder:32b-instruct-q8_0 ( ctx: 16384, max_tokens: 8192 )
- <kbd>deepseekliteq8</kbd>: ollama.com/library/deepseek-coder-v2:16b-lite-instruct-q8_0
- <kbd>deepseeklite</kbd>: deepseek-coder-v2:latest

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Inference

Available models:

- <kbd>command-r7b</kbd>: command-r7b:latest
- <kbd>llama8b</kbd>: llama3.1:latest
- <kbd>qwen8b</kbd>: qwen3:8b
- <kbd>granite8b</kbd>: granite3.3:8b
- <kbd>nemo</kbd>: mistral-nemo:latest
- <kbd>qwen14b</kbd>: qwen3:14b
- <kbd>phi14b</kbd>: phi4:latest
- <kbd>mistral-small</kbd>: mistral-small:latest ( ctx: 16384, max_tokens: 8192 )
- <kbd>gemma27b</kbd>: gemma3:27b ( ctx: 16384, max_tokens: 8192 )
- <kbd>qwen32b</kbd>: qwen3:32b ( ctx: 16384, max_tokens: 8192 )
- <kbd>qwen72b</kbd>: qwen2.5:72b ( ctx: 16384, max_tokens: 8192 )
- <kbd>mistral-large</kbd>: Mistral-Large-Instruct-2411-IQ4_XS ( ctx: 16384, max_tokens: 8192 )

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Thinking

- <kbd>deepscaler</kbd>: deepscaler:1.5b-preview-q8_0
- <kbd>exaone2b</kbd>: exaone-deep:2.4b-q8_0
- <kbd>smallthinker</kbd>: smallthinker:3b-preview-q8_0
- <kbd>exaone8b</kbd>: exaone-deep:latest
- <kbd>deephermes</kbd>: hf.co/bartowski/NousResearch_DeepHermes-3-Llama-3-8B-Preview-GGUF:Q6_K_L
- <kbd>qwq</kbd>: qwq:latest
- <kbd>qwq-q8</kbd>: qwq:32b-q8_0

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Mini

- <kbd>llama1b</kbd>: llama3.2:1b-instruct-q8_0
- <kbd>granite2b</kbd>: granite3.3:2b
- <kbd>qwen3b</kbd>: qwen2.5:3b-instruct-q8_0
- <kbd>llama3b</kbd>: llama3.2:3b-instruct-q8_0
- <kbd>qwen4b</kbd>: qwen3:4b
- <kbd>phi4mini</kbd>: phi4-mini:3.8b-q8_0
- <kbd>nemotron4b</kbd>: nemotron-mini:4b-instruct-q8_0
- <kbd>gemma4b</kbd>: gemma3:4b-it-qat

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Multilingual

- <kbd>granite2b</kbd>: granite3.2:2b-instruct-q8_0
- <kbd>granite8b</kbd>: granite3.3:8b
- <kbd>aya8b</kbd>: aya-expanse:latest
- <kbd>aya8bq6</kbd>: aya-expanse:8b-q6_K
- <kbd>gemma12b</kbd>: gemma3:12b
- <kbd>gemma27b</kbd>: gemma3:27b
- <kbd>aya35b</kbd>: aya:35b

Context:

- Default context: **16k**
- Default max tokens: **8K**

### Tools

Available models:

- <kbd>granite2b-tools</kbd>: granite3.3:2b
- <kbd>qwen3b-tools</kbd>: qwen2.5:3b-instruct-q8_0
- <kbd>qwen4b-tools</kbd>: qwen3:4b
- <kbd>qwen8b-tools</kbd>: qwen3:8b
- <kbd>granite8b-tools</kbd>: granite3.3:8b
- <kbd>qwen14b-tools</kbd>: qwen3:14b
- <kbd>mistral-small-tools</kbd>: mistral-small:latest
- <kbd>mistral-small-vision-tools</kbd>: mistral-small3.1:24b
- <kbd>qwen30b-tools</kbd>: qwen3:30b
- <kbd>qwen32b-tools</kbd>: qwen3:32b ( ctx: 16384, max_tokens: 8192 )
- <kbd>qwen72b-tools</kbd>: qwen2.5:72b ( ctx: 16384, max_tokens: 8192 )
- <kbd>mistral-large-tools</kbd>: Mistral-Large-Instruct-2411-IQ4_XS ( ctx: 16384, max_tokens: 8192 )

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Vision

Available models:

- <kbd>granite2b-vision</kbd>: granite3.2-vision:2b-q8
- <kbd>gemma4b-vision</kbd>: gemma3:4b-it-q8_0
- <kbd>llama3-vision</kbd>: llama3.2-vision:latest
- <kbd>minicpm-vision</kbd>: minicpm-v:8b-2.6-q8_0
- <kbd>gemma12b-vision</kbd>: gemma3:12b
- <kbd>mistral-small-vision</kbd>: mistral-small3.1:24b
- <kbd>gemma27b</kbd>: gemma3:27b

Context:

- Default context: **32k**
- Default max tokens: **8K**

<a href="javascript:openLink('/terminal_client/plugins/inference')">Next: Inference</a>