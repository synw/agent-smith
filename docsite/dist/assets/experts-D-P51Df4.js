import{d as w,r as b,e as j,o as r,c as d,v as g,a as t,g as l,f as s,H as n,M as o,h as a,w as T,F as h,x as i,i as A,t as x,y as k}from"./index-56o_VcVb.js";import{s as E}from"./textarea.esm-BGeGFH_f.js";const I={class:"flex flex-col space-y-5 mt-5"},L={class:"flex flex-col space-y-5 mt-5"},M={class:"flex flex-col space-y-5 mt-5"},B={key:0},S={class:"flex flex-row space-x-2"},N=["disabled"],V={class:"font-light"},q={class:"flex flex-col space-y-5 mt-5"},C={class:"flex flex-col space-y-5 mt-5"},Q=`import { useLmBackend, useLmExpert } from "@agent-smith/brain";

const model = "llama3.1:latest";
const ctx = 8192;
const templateName = "llama3";

const backend = useLmBackend({
    name: "ollama",
    localLm: "ollama",
});

const expert = useLmExpert({
    name: "llama",
    backend: backend,
    template: templateName,
    model: { name: model, ctx: ctx },
});`,F=`import { useAgentBrain } from "@agent-smith/brain";

const brain = useAgentBrain([backend], [expert]);`,R="brain.addExpert(backend);",$='brain.removeExpert("llama") // expert.name;',K=`expert.checkStatus();

console.log("Is thinking:", expert.state.get().isThinking);
console.log("Is emiting:", expert.state.get().isEmiting);
console.log("Is thinking:", expert.state.get().status);`,O=`await expert.think(
    "Write a short list of the planets names of the solar system (...)", {
    temperature: 0,
    min_p: 0.05,
    max_tokens: 200,
})`,U="expert.abortThinking()",z="expert.loadModel()",D=`expert.setModel({ name: "llama3.1:latest", ctx: 8192 });
expert.loadModel()`,G=`expert.setModel({ 
    name: "Qwen2.5-0.5B-Instruct",
    ctx: 32768,
    extra: { 
        urls: "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf"
    }
});
expert.loadModel()`,H='expert.setTemplate("chatml")',Y=w({__name:"experts",setup(W){let m,v;const c=b(!1),p=b("Write a short list of the planets names of the solar system. Important: return only a markdown list.");async function y(){console.log(i.ex.template.prompt(p.value)),await i.think(p.value,{temperature:0,min_p:.05,max_tokens:200,extra:{raw:!0}},{verbose:!0})}function f(){c.value=i.experts.length>0,c.value&&(i.ex.checkStatus(),m=k(i.ex.state),v=k(i.ex.backend.stream))}return j(()=>f()),(J,e)=>(r(),d("div",null,[e[22]||(e[22]=g('<div class="prosed"><h1>Experts</h1></div><div class="flex flex-col space-y-5 mt-5"><div>Experts are attached to the brain. An expert is:</div></div><div class="prosed"><ul><li><span class="font-semibold">A backend</span>: a remote or local inference server, or the browser </li><li><span class="font-semibold">A model</span>: a defined language model </li><li><span class="font-semibold">A template</span>: a template format to use with the model </li></ul></div>',3)),t("div",I,[e[4]||(e[4]=t("div",null,"An expert can run inference queries.",-1)),e[5]||(e[5]=t("div",null,"Initialize an expert:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:Q,lang:"ts"},null,8,["hljs"])]),e[6]||(e[6]=t("div",null,"Experts can be attached to the brain: at initialization time:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:F,lang:"ts"},null,8,["hljs"])]),e[7]||(e[7]=t("div",null,"At anytime:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:R,lang:"ts"},null,8,["hljs"])]),e[8]||(e[8]=t("div",null,"Remove an expert:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:$,lang:"ts"},null,8,["hljs"])])]),e[23]||(e[23]=g('<div class="prosed"><h2>State</h2></div><div class="flex flex-col space-y-5 mt-5"><div>The expert have a state. Available properties:</div></div><div class="prosed"><ul><li><kbd>isStreaming</kbd>: <code>true</code> if the model is emiting token </li><li><kbd>isThinking</kbd>: <code>true</code> if either the model is emiting token or ingesting a prompt </li><li><kbd>status</kbd>: the expert status: possible values: <ul><li><span class="font-semibold">unavailable</span>: the backend is down or the model can not be loaded</li><li><span class="font-semibold">available</span>: the backend is up and the model can be loaded</li><li><span class="font-semibold">ready</span>: the backend is up and the model is loaded</li></ul></li></ul></div>',3)),t("div",L,[e[9]||(e[9]=t("div",null,[a("Use "),t("kbd",null,"checkStatus"),a(" to update the expert's status:")],-1)),t("div",null,[l(s(o),{hljs:s(n),code:K,lang:"ts"},null,8,["hljs"])])]),e[24]||(e[24]=t("div",{class:"prosed"},[t("h2",null,"Inference")],-1)),t("div",M,[e[11]||(e[11]=t("div",null,"Once the expert ready it can run inference queries.",-1)),c.value?(r(),d(h,{key:1},[s(m).status=="ready"?(r(),d(h,{key:0},[l(s(E),{class:"w-[50rem] mt-3",modelValue:p.value,"onUpdate:modelValue":e[1]||(e[1]=u=>p.value=u),rows:2},null,8,["modelValue"]),t("div",S,[t("button",{class:"btn semilight",onClick:e[2]||(e[2]=u=>y()),disabled:s(m).isThinking},"Run the query",8,N),s(m).isThinking?(r(),d("button",{key:0,onClick:e[3]||(e[3]=u=>s(i).ex.abortThinking())},"Abort")):A("",!0)]),t("div",null,[t("pre",V,x(s(v)),1)])],64)):(r(),d(h,{key:1},[a("expert not ready: "+x(s(m).status),1)],64))],64)):(r(),d("div",B,[e[10]||(e[10]=t("div",{class:"text-lg font-medium"},"Configure an expert for the interactive demo:",-1)),l(T,{class:"p-3 mt-3 border rounded-md",onEnd:e[0]||(e[0]=u=>f())})])),e[12]||(e[12]=t("div",null,"Run an inference query:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:O,lang:"ts"},null,8,["hljs"])]),e[13]||(e[13]=t("div",null,"Abort a running inference query:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:U,lang:"ts"},null,8,["hljs"])]),e[14]||(e[14]=t("div",null,"Inference parameters:",-1))]),e[25]||(e[25]=g('<div class="prosed"><div><ul><li><strong>stream</strong>: (<code>boolean</code>) Indicates if results should be streamed progressively.</li><li><strong>model</strong>: (<code>ModelConf</code>) The model configuration details for inference. </li><li><strong>template</strong>: (<code>string</code>) The template to use, for the backends that support it.</li><li><strong>max_tokens</strong>: (<code>number</code>) The max number of tokens to emit.</li><li><strong>top_k</strong>: (<code>number</code>) Limits the result set to the top K results.</li><li><strong>top_p</strong>: (<code>number</code>) Filters results based on cumulative probability. </li><li><strong>min_p</strong>: (<code>number</code>) The minimum probability for a token to be considered, relative to the probability of the most likely token.</li><li><strong>temperature</strong>: (<code>number</code>) Adjusts randomness in sampling; higher values mean more randomness.</li><li><strong>repeat_penalty</strong>: (<code>number</code>) Adjusts penalty for repeated tokens.</li><li><strong>tfs</strong>: (<code>number</code>) Set the tail free sampling value.</li><li><strong>stop</strong>: (<code>Array&lt;string&gt;</code>) List of stop words or phrases to halt predictions. </li><li><strong>grammar</strong>: (<code>string</code>) The gnbf grammar to use for grammar-based sampling.</li><li><strong>images</strong>: (<code>Array&lt;string&gt;</code>) The base64 images data (for multimodal models). </li><li><strong>extra</strong>: (<code>Record&lt;string, any&gt;</code>) Extra parameters to include in the payload </li></ul></div></div><div class="flex flex-col space-y-5 mt-5"><div>Detailled <a href="https://synw.github.io/locallm/types/interfaces/InferenceParams.html">inference params api doc</a></div></div><div class="prosed"><h2>Models</h2></div>',3)),t("div",q,[e[15]||(e[15]=t("div",null,[a("An expert is associated with a model. If the backend is Ollama or the browser the expert will have to load the model, and can change model later. Note: when calling "),t("code",null,"think"),a(" the model will be automatically loaded if it is not loaded yet. The Koboldcpp and Llama.cpp backends can not switch models, so the initial model is always loaded. ")],-1)),e[16]||(e[16]=t("div",null,"Load a the expert's model:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:z,lang:"ts"},null,8,["hljs"])]),e[17]||(e[17]=t("div",null,"Load another model in Ollama:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:D,lang:"ts"},null,8,["hljs"])]),e[18]||(e[18]=t("div",null,[a("Load another model in the browser: note the "),t("code",null,"extra.urls"),a(" param:")],-1)),t("div",null,[l(s(o),{hljs:s(n),code:G,lang:"ts"},null,8,["hljs"])])]),e[26]||(e[26]=t("div",{class:"prosed"},[t("h2",null,"Templates")],-1)),t("div",C,[e[19]||(e[19]=t("div",null,"Each expert is associated with a template, corresponding to the model used. To change a template:",-1)),t("div",null,[l(s(o),{hljs:s(n),code:H,lang:"ts"},null,8,["hljs"])]),e[20]||(e[20]=t("div",null,[a("The templates are managed using the "),t("a",{href:"https://github.com/synw/modprompt"},"Modprompt"),a(" library. They can be used for few shots prompting, history of turns, system messages and so on: check the "),t("a",{href:"javascript:openLink('/the_brain/templates/basics')"},"templates"),a(" section. ")],-1)),e[21]||(e[21]=t("div",{class:"pt-5"},[t("a",{href:"javascript:openLink('/the_brain/brain')"},"Next: the brain")],-1))])]))}});export{Y as default};
