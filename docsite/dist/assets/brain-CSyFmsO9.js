import{d,c as i,o as r,a as l,g as t,f as n,H as s,M as a,i as o}from"./index-ruRH7gpx.js";const u={class:"flex flex-col space-y-5 mt-5"},c={class:"flex flex-col space-y-5 mt-5"},m={class:"flex flex-col space-y-5 mt-5"},p={class:"flex flex-col space-y-5 mt-5"},b={class:"flex flex-col space-y-5 mt-5"},v=`import { useLmBackend, useAgentBrain } from "@agent-smith/brain";

const localBackend = useLmBackend({
    name: "ollama",
    localLm: "ollama",
});

const remoteBackend = useLmBackend({
    name: "remote_backend",
    serverUrl: "https://myurl.com",
    apiKey: "xyz",
    providerType: "koboldcpp",
});

const brain = useAgentBrain([localBackend, browserBackend]);`,f='brain.removeBackend("ollama") // backend.name',x="brain.addBackend(localBackend)",k='const backend = brain.backend("remote_backend")',g="const backends = brain.backends",h=`import { useLmBackend, useLmExpert } from "@agent-smith/brain";

const backend = useLmBackend({
    name: "ollama",
    localLm: "ollama",
});

const expert = useLmExpert({
    name: "llama",
    backend: backend,
    template: "llama3",
    model: { name: "llama3.1:latest", ctx: 8192 },
});`,j=`import { useAgentBrain } from "@agent-smith/brain";

const brain = useAgentBrain([backend], [expert]);`,w="brain.addExpert(backend);",y='brain.removeExpert("llama") // expert.name;',B='const expert = brain.expert("llama");',L="const experts = brain.experts;",E=`brain.setDefaultExpert("llama");
// or
brain.setDefaultExpert(expert)`,T="const ex = brain.ex;",G="const usableExperts = brain.workingExperts();",M=`const info = await brain.backendsForModelsInfo();
console.log("Models for a backend named ollama:", info.ollama);`,A='const backendName = brain.getBackendForModel("llama3.1:latest")',I='const expert = brain.getExpertForModel("llama3.1:latest")',F='const expert = brain.getOrCreateExpertForModel("llama3.1:latest")',N='const res = await brain.think("my prompt...", {temperature: 0.5})',R='const res = await brain.thinkx("llama", "my prompt...")',U="const res = await brain.abortThinking();",q="const backends = await brain.discoverLocal();",z="const isUp = await brain.discover();",D=`const isUp = await brain.initLocal();
const isUp = await brain.init();`,V=d({__name:"brain",setup(S){return(K,e)=>(r(),i("div",null,[e[27]||(e[27]=l("div",{class:"prosed"},[l("h1",null,"Brain")],-1)),e[28]||(e[28]=l("div",{class:"flex flex-col space-y-5 mt-5"},[l("div",null,"The brain is an interface to manage multiple experts and backends.")],-1)),e[29]||(e[29]=l("div",{class:"prosed"},[l("h2",null,"Backends")],-1)),l("div",u,[e[0]||(e[0]=l("div",null,"Initialize the brain with backends:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:v,lang:"ts"},null,8,["hljs"])]),e[1]||(e[1]=l("div",null,"Remove a backend:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:f,lang:"ts"},null,8,["hljs"])]),e[2]||(e[2]=l("div",null,"Add a backend:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:x,lang:"ts"},null,8,["hljs"])]),e[3]||(e[3]=l("div",null,"Get a backend:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:k,lang:"ts"},null,8,["hljs"])]),e[4]||(e[4]=l("div",null,"Get all backends:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:g,lang:"ts"},null,8,["hljs"])])]),e[30]||(e[30]=l("div",{class:"prosed"},[l("h2",null,"Experts")],-1)),l("div",c,[e[5]||(e[5]=l("div",null,"Experts can be declared at initialization time or later.",-1)),l("div",null,[t(n(a),{hljs:n(s),code:h,lang:"ts"},null,8,["hljs"])]),e[6]||(e[6]=l("div",null,"Declare experts at initialization time:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:j,lang:"ts"},null,8,["hljs"])]),e[7]||(e[7]=l("div",null,"Add an expert:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:w,lang:"ts"},null,8,["hljs"])]),e[8]||(e[8]=l("div",null,"Remove an expert:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:y,lang:"ts"},null,8,["hljs"])]),e[9]||(e[9]=l("div",null,"Get an expert (will throw an error if not found):",-1)),l("div",null,[t(n(a),{hljs:n(s),code:B,lang:"ts"},null,8,["hljs"])]),e[10]||(e[10]=l("div",null,"Get all experts:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:L,lang:"ts"},null,8,["hljs"])]),e[11]||(e[11]=l("div",null,"The brain has a default expert. It will be set automatically on the first expert added. To set the default expert: ",-1)),l("div",null,[t(n(a),{hljs:n(s),code:E,lang:"ts"},null,8,["hljs"])]),e[12]||(e[12]=l("div",null,"Get the default expert:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:T,lang:"ts"},null,8,["hljs"])]),e[13]||(e[13]=l("div",null,[o("Get a list of usable expert (with status "),l("code",null,"ready"),o(" or "),l("code",null,"available"),o("):")],-1)),l("div",null,[t(n(a),{hljs:n(s),code:G,lang:"ts"},null,8,["hljs"])])]),e[31]||(e[31]=l("div",{class:"prosed"},[l("h2",null,"Models")],-1)),l("div",m,[e[14]||(e[14]=l("div",null,"Some backends can manage multiple models (Ollama and the browser) and some only one (Koboldcpp and Llama.cpp). Each of these backends can provide information about the models it supports. To get a list of models available per backend: ",-1)),l("div",null,[t(n(a),{hljs:n(s),code:M,lang:"ts"},null,8,["hljs"])]),e[15]||(e[15]=l("div",null,[o("Get what backend is available for a given model (will return "),l("code",null,"null"),o(" if not found):")],-1)),l("div",null,[t(n(a),{hljs:n(s),code:A,lang:"ts"},null,8,["hljs"])]),e[16]||(e[16]=l("div",null,[o("Get an expert for a given model (will return "),l("code",null,"null"),o(" if not found):")],-1)),l("div",null,[t(n(a),{hljs:n(s),code:I,lang:"ts"},null,8,["hljs"])]),e[17]||(e[17]=l("div",null,[o("Get or create an expert for a given model (will return "),l("code",null,"null"),o(" if it can not be created - ex: no backend exists for this model). If a backend for the given model exists, it will be used and an expert named as the model name will be created. Useful to create experts on the fly for automated tasks.")],-1)),l("div",null,[t(n(a),{hljs:n(s),code:F,lang:"ts"},null,8,["hljs"])])]),e[32]||(e[32]=l("div",{class:"prosed"},[l("h2",null,"Inf√©rence")],-1)),l("div",p,[e[18]||(e[18]=l("div",null,"There are some convenience methods to use the default expert to run inference queries. Run a query using the defaut expert: ",-1)),l("div",null,[t(n(a),{hljs:n(s),code:N,lang:"ts"},null,8,["hljs"])]),e[19]||(e[19]=l("div",null,"Run a query using a given expert:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:R,lang:"ts"},null,8,["hljs"])]),e[20]||(e[20]=l("div",null,"Abort a running query:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:U,lang:"ts"},null,8,["hljs"])]),e[21]||(e[21]=l("div",null,[o("Refer the the "),l("a",{href:"javascript:openLink('/the_brain/experts')"},"experts doc"),o(" for more details on inference. ")],-1))]),e[33]||(e[33]=l("div",{class:"prosed"},[l("h2",null,"Auto discovery")],-1)),l("div",b,[e[22]||(e[22]=l("div",null,"The brain has some function to ping the declared backends and experts and check their state. It can also auto discover local backend: ",-1)),l("div",null,[t(n(a),{hljs:n(s),code:q,lang:"ts"},null,8,["hljs"])]),e[23]||(e[23]=l("div",null,[o("This will check if any Llama.cpp, Koboldcpp or Ollama is running locally (with their defaults settings). It will add the backends to the brain, unless the "),l("kbd",null,"setState"),o(" optional first param is "),l("code",null,"false"),o(" (to just check the backends and not auto add them to the brain). Second parameter is for the verbosity, set it to "),l("code",null,"true"),o(" for a verbose console output. ")],-1)),e[24]||(e[24]=l("div",null,"Discover the declared backends: it will check the existing backend and update their state:",-1)),l("div",null,[t(n(a),{hljs:n(s),code:z,lang:"ts"},null,8,["hljs"])]),e[25]||(e[25]=l("div",null,[o("There are similar "),l("kbd",null,"init"),o(" functions that will in addition run "),l("kbd",null,"backendsForModelsInfo"),o(" to check the models's state.")],-1)),l("div",null,[t(n(a),{hljs:n(s),code:D,lang:"ts"},null,8,["hljs"])]),e[26]||(e[26]=l("div",{class:"pt-5"},[l("a",{href:"javascript:openLink('/the_brain/grammars')"},"Next: grammars")],-1))])]))}});export{V as default};
