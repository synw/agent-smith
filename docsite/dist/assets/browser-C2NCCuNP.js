import{g as S,f as I,C as $,D as V,o,c as i,E as p,G as C,i as f,a as s,h as g,t as w,d as P,r as c,e as h,u as r,H as y,M as B,n as T,P as A}from"./index-BMtjbDYc.js";import{s as F}from"./textarea.esm-CyVB3fZ-.js";const v=S([I({name:"browser",localLm:"browser"})]);var j={root:function(d){var u=d.instance;return["p-progressbar p-component",{"p-progressbar-determinate":u.determinate,"p-progressbar-indeterminate":u.indeterminate}]},container:"p-progressbar-indeterminate-container",value:"p-progressbar-value p-progressbar-value-animate",label:"p-progressbar-label"},q=$.extend({name:"progressbar",classes:j}),E={name:"BaseProgressBar",extends:V,props:{value:{type:Number,default:null},mode:{type:String,default:"determinate"},showValue:{type:Boolean,default:!0}},style:q,provide:function(){return{$parentInstance:this}}},L={name:"ProgressBar",extends:E,inheritAttrs:!1,computed:{progressStyle:function(){return{width:this.value+"%",display:"flex"}},indeterminate:function(){return this.mode==="indeterminate"},determinate:function(){return this.mode==="determinate"}}},G=["aria-valuenow"];function H(a,d,u,t,m,l){return o(),i("div",p({role:"progressbar",class:a.cx("root"),"aria-valuemin":"0","aria-valuenow":a.value,"aria-valuemax":"100"},a.ptmi("root")),[l.determinate?(o(),i("div",p({key:0,class:a.cx("value"),style:l.progressStyle},a.ptm("value")),[a.value!=null&&a.value!==0&&a.showValue?(o(),i("div",p({key:0,class:a.cx("label")},a.ptm("label")),[C(a.$slots,"default",{},function(){return[g(w(a.value+"%"),1)]})],16)):f("",!0)],16)):f("",!0),l.indeterminate?(o(),i("div",p({key:1,class:a.cx("container")},a.ptm("container")),[s("div",p({class:a.cx("value")},a.ptm("value")),null,16)],16)):f("",!0)],16,G)}L.render=H;const N={class:"flex flex-col space-y-5 mt-5"},R=["disabled"],U={key:0},z={class:"flex flex-row space-x-3"},D=["disabled"],Q=`import { useLmExpert, useAgentBrain } from "@agent-smith/brain";

const brain = useAgentBrain([
    useLmExpert({
        name: "smollm-360m",
        localLm: "browser",
    }),
]);

export { brain }`,W=`const onModelLoading = (st) => {
    console.log(st.percent, "%")
}

async function loadModel() {
    await brain.ex.lm.loadBrowsermodel(
        "smollm-360m",
        "https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF/resolve/main/smollm-360m-instruct-add-basics-q8_0.gguf",
        2048,
        onModelLoading,
    );
    await brain.discoverBrowser(true);
}`,Y=P({__name:"browser",setup(a){const d=c(""),u=c(0),t=c(!1),m=c(!1),l=c("List the orbital periods of the planets of the solar system"),k=n=>{console.log(n.percent,"%"),u.value=n.percent};async function x(){await v.ex.lm.loadBrowsermodel("smollm-360m","https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF/resolve/main/smollm-360m-instruct-add-basics-q8_0.gguf",2048,k),await v.discoverBrowser(!0),v.ex.setOnToken(n=>d.value=n),t.value=!0}async function M(){m.value=!0;const n=new A("chatml").replaceSystem("You are an AI assistant. Important: always use json to respond").prompt(l.value),e=await v.think(n,{temperature:0,min_p:.05,max_tokens:512});m.value=!1,console.log(e.stats)}return(n,e)=>(o(),i("div",null,[e[10]||(e[10]=s("div",{class:"prosed"},[s("h1",null,"In browser inference")],-1)),s("div",N,[e[4]||(e[4]=s("div",null,[g("Run inference queries locally in the browser. It uses "),s("a",{href:"https://github.com/ngxson/wllama"},"Wllama"),g(" to run Llama.cpp in the browser. It supports only cpu inference for now.")],-1)),e[5]||(e[5]=s("div",{class:"prosed"},[s("h2",null,"Initialize")],-1)),s("div",null,[h(r(B),{hljs:r(y),code:Q,lang:"ts"},null,8,["hljs"])]),e[6]||(e[6]=s("div",{class:"prosed"},[s("h2",null,"Load a model")],-1)),e[7]||(e[7]=s("div",null,"Let's load the Smollm 360m instruct model from HuggingFace:",-1)),s("div",null,[h(r(B),{hljs:r(y),code:W,lang:"ts"},null,8,["hljs"])]),e[8]||(e[8]=s("div",null,[g("The model will be downloaded once and placed in the cache for later local load. Important: the model files must not exceed 512 megabytes to be able to stay in the cache. For models bigger than 512M split the gguf files using Llama.cpp (ref: "),s("a",{href:"https://github.com/ngxson/wllama?tab=readme-ov-file#split-model"},"Wllama doc"),g(") ")],-1)),s("div",null,[s("button",{class:"btn light",onClick:e[0]||(e[0]=b=>x()),disabled:t.value},"Load model",8,R)]),t.value?f("",!0):(o(),i("div",U,[h(r(L),{value:u.value},null,8,["value"])])),s("div",null,"Is model loaded: "+w(t.value),1),e[9]||(e[9]=s("div",{class:"prosed"},[s("h2",null,"Run an inference query")],-1)),s("div",null,[h(r(F),{class:"w-[50rem] mt-3",modelValue:l.value,"onUpdate:modelValue":e[1]||(e[1]=b=>l.value=b),rows:1},null,8,["modelValue"])]),s("div",z,[s("button",{class:T(["btn",t.value?"success":"light"]),onClick:e[2]||(e[2]=b=>M()),disabled:!t.value||m.value},"Run inference",10,D),m.value?(o(),i("button",{key:0,class:"btn danger",onClick:e[3]||(e[3]=b=>r(v).abortThinking())},"Abort")):f("",!0)]),s("div",null,w(d.value),1)])]))}});export{Y as default};
