## Models

*Note*: this section is only relevant for backends that allow model switching: Ollama and Openai api compatible
endpoints that enable it.

The specified model for the task is used by default. To override it an use another model, first initialize aa
agent for the task:

```ts
import { Task } from "@agent-smith/task";
import { Lm } from "@locallm/api";
import { Agent } from "@agent-smith/agent";

const lm = new Lm({
    providerType: "ollama",
    serverUrl: "http://localhost:11434",
    onToken: (t) => process.stdout.write(t),
});
const agent = new Agent(lm);
```

Define an alternative model to use:

```ts
const model = { name: "gemma3n:latest", template: "gemma" };
```

Run a task with this model:

```ts
const ymlTaskDef = `name: infer
description: Run a raw inference query
prompt: |-
    {prompt}
ctx: 16384
model:
  name: qwen3:4b-instruct
  template: chatml
inferParams:
  top_p: 0.8
  top_k: 20
  min_p: 0
  temperature: 0.6
  max_tokens: 8192`;
const conf = { model: model };
await task.run({ prompt: "lorem ipsum.." }, conf);
```

## Specifying alternative model

Alternative models can be specified in a task. Example:

```yaml
name: infer
description: Run a raw inference query
prompt: |-
    {prompt}
ctx: 16384
model:
  name: qwen3:4b-instruct
  template: chatml
models:
    granite:
        name: granite3.1-dense:2b-instruct-q8_0
        template: chatml
    aya8b:
        name: aya-expanse:8b-q6_K
        ctx: 8192
        template: command-r
```

As the model is specified in the task you can use the `modelname` conf parameter in the
request:

```ts
const conf = { modelname: "aya8b" };
await task.run({ prompt: "lorem ipsum.." }, conf);
```

## Overriding inference parameters in model definitions

You can override some parameters when defining alternative models:

```yaml
# ...
ctx: 16384
model:
  name: qwen3:4b-instruct
  template: chatml
model:
    name: qwen3:4b-instruct
    template: chatml
models:  
    gemma3n:
        name: gemma3n:latest
        template: gemma
        ctx: 32768
        #afterSystem: "\nAppend this message to system message"
        #system: "A new system message replacing the original"
        assistant: "Sure, let's" # starts the model's response
        inferParams:
            max_tokens: 16384
            top_p: 0.95
inferParams:
    top_p: 0.8
    top_k: 20
    min_p: 0
    temperature: 0.6
    max_tokens: 8192
```

In this example when using gemma3n it will use bigger context and max tokens emitted value. The
default task's inference params are merged with the modifications sp√©cified for the model.


<a href="javascript:openLink('/libraries/task/templates')">Next: templates</a>