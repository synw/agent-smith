# Templates

*Note*: for the non openai providers we use manual templating: Llama.cpp, Koboldcpp, Ollama. The openai
compatible api uses server side templates and is not concerned by this section.

## Initialize an agent

Initialize a local Llama.cpp agent:

```js
import { Agent } from "@agent-smith/agent";

const lm = new Lm({
    providerType: "llamacpp",
    serverUrl: "http://localhost:8080",
    onToken: (t) => process.stdout.write(t),
});
const agent = new Agent(lm);
```

Create some tools:

```js
function run_get_current_weather(args) {
    return '{ "temp": 20.5, "weather": "rain" }'
}

const get_current_weather = {
    "name": "get_current_weather",
    "description": "Get the current weather",
    "arguments": {
        "location": {
            "description": "The city and state, e.g. San Francisco, CA",
            "required": true
        }
    },
    execute: run_get_current_weather
};
```

## Run the agent with a template

```js
import { PromptTemplate } from "modprompt";

const template = new PromptTemplate("chatml-tools")
    .afterSystem("\nYou are and autonomous agent: feel free to use your tools.");

const _prompt = `I am landing in Barcelona soon: I plan to reach my hotel 
and then go for outdoor sport. How are the conditions in the city?`;
const inferenceParams = {
    stream: true,
    model: { 
        name: "Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL",
        ctx: 32768,
    },
    temperature: 0.4,
    top_k: 40,
    top_p: 0.95,
    min_p: 0,
    max_tokens: 16384,
};
const options = {
    tools: [get_current_weather]
}

await agent.run(_prompt, inferenceParams, options, template);
```


<a href="javascript:openLink('/libraries/agent/supervision')">Next: supervised agent</a>