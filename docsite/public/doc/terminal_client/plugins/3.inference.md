# Inference

![pub package](https://img.shields.io/npm/v/@agent-smith/feat-inference)

Basic inference feature

## Install

```bash
npm i -g @agent-smith/feat-inference
```

Add the plugin and it's dependency to your `config.yml` file and run the `conf` command to update the cli.

```yml
plugins:
  - "@agent-smith/feat-inference"
  - "@agent-smith/feat-models"
```

```bash
lm conf ~/path/to/config.yml
```

## Usage

Commands:

- <kbd>infer</kbd>: run an inference query
- <kbd>think</kbd>: run an inference query using a thinking model
- <kbd>q</kbd>: shortcut to run an inference query

## Inference

Run an inference query:

```bash
lm infer "list the planets of the solar system"
```

Default model: `nemotron-mini:4b-instruct-q8_0` with 32k context.

Available <a href="javascript:openLink('/terminal_client/plugins/models')">models</a>: see
the `inference` models collection

To run a query with a different model size:

```bash
lm infer "list the planets of the solar system" s=24b
```

This will run the query with `mistral-small:latest`16K context

Shortcut to run an inference query without the need of quotes: use <kbd>q</kbd>:

```bash
lm q list the planets of the solar system s=3b
```

Default inference params:

- *top_k*: 0
- *top_p*: 1
- *min_p*: 0.05
- *temperature*: 0.4

## Inference with thinking

Default model: `smallthinker:3b-preview-q8_0` with 32k context.

Available <a href="javascript:openLink('/terminal_client/plugins/models')">models</a>: see
the `thinking` models collection

Run an inference query:

```bash
lm think "discuss the interest of counting the number of 'r' characters in the word 'strawberry'" s=32b
```

This will run the query using QwQ 32b

Default inference params:

- *top_k*: 0
- *top_p*: 1
- *min_p*: 0.05
- *temperature*: 0.4

## Inference parameters

To override the default inference parameters:

```bash
lm infer "list the planets of the solar system" ip="temperature:0.8,min_p:0.1"
```

Fo the full list of available parameters see <a href="javascript:openLink('/lm_task/specification')">LmTask spec</a>

## Options

To visualize the tokens for an inference query use the *-t* option:

```bash
lm q list the planets of the solar system -t
```

To view the prompt template and inference params info use the *-v* option:

```bash
lm q list the planets of the solar system -v
```

To continue chatting with the model after the response use the *-c* option:

```bash
lm q list the planets of the solar system -c
```

<a href="javascript:openLink('/terminal_client/plugins/vision')">Next: Vision</a>