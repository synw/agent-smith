# Models

![pub package](https://img.shields.io/npm/v/@agent-smith/feat-models)

Collections of models to use for inference tasks

## Install

```bash
npm i @agent-smith/feat-models
```

Add the plugin to your config file and run the `conf`Â command to update the cli

## Usage

To use a models set in a task:

```yaml
description: Run an inference query with a thinking model
prompt: |-
    {prompt}
modelset: 
  name: thinking
  default: 32b
```

## Collections

Available models collections:

- <kbd>inference</kbd>: instruct models of various sizes
- <kbd>thinking</kbd>: a collection of thinking models
- <kbd>mini</kbd>: small models of 4b or less
- <kbd>vision</kbd>: vision models

### Inference

Available models:

- <kbd>1b</kbd>: llama3.2:1b-instruct-q8_0
- <kbd>2b</kbd>: granite3.2:2b-instruct-q8_0
- <kbd>3b</kbd>: qwen2.5:3b-instruct-q8_0
- <kbd>llama3b</kbd>: llama3.2:3b-instruct-q8_0
- <kbd>3.8b</kbd>: phi4-mini:3.8b-q8_0
- <kbd>4b</kbd>: nemotron-mini:4b-instruct-q8_0
- <kbd>7b</kbd>: command-r7b:latest
- <kbd>8b</kbd>: llama3.1:latest
- <kbd>12b</kbd>: mistral-nemo:latest
- <kbd>14b</kbd>: qwen2.5:14b
- <kbd>phi14b</kbd>: phi4:latest
- <kbd>24b</kbd>: mistral-small:latest ( ctx: 16384, max_tokens: 8192 )
- <kbd>27b</kbd>: gemma3:27b ( ctx: 16384, max_tokens: 8192 )
- <kbd>32b</kbd>: qwen2.5:32b ( ctx: 16384, max_tokens: 8192 )
- <kbd>72b</kbd>: qwen2.5:72b ( ctx: 16384, max_tokens: 8192 )

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Thinking

- <kbd>1.5b</kbd>: deepscaler:1.5b-preview-q8_0
- <kbd>3b</kbd>: smallthinker:3b-preview-q8_0
- <kbd>4b</kbd>: exaone-deep:2.4b-q8_0
- <kbd>7.8b</kbd>: exaone-deep:latest
- <kbd>8b</kbd>: hf.co/bartowski/NousResearch_DeepHermes-3-Llama-3-8B-Preview-GGUF:Q6_K_L
- <kbd>32b</kbd>: qwq:32b-q8_0

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Mini

- <kbd>granite2b</kbd>: granite3.2:2b-instruct-q8_0 (ctx: 32768, max_tokens: 16384)
- <kbd>qwen3b</kbd>: qwen2.5:3b-instruct-q8_0
- <kbd>llama3b</kbd>: llama3.2:3b-instruct-q8_0
- <kbd>phi4b</kbd>: phi4-mini:3.8b-q8_0
- <kbd>nemotron4b</kbd>: nemotron-mini:4b-instruct-q8_0
- <kbd>gemma4b</kbd>: gemma3:4b-it-q8_0

Context:

- Default context: **32k**
- Default max tokens: **16K**

### Vision

- <kbd>granite2b</kbd>: granite3.2-vision:2b-q8_0
- <kbd>llama3</kbd>: llama3.2-vision:latest
- <kbd>minicpm</kbd>: minicpm-v:8b-2.6-q8_0

Context:

- Default context: **32k**
- Default max tokens: **8K**

<a href="javascript:openLink('/terminal_client/plugins/inference')">Next: Inference</a>
