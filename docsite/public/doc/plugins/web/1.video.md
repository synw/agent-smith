# Video

![pub package](https://img.shields.io/npm/v/@agent-smith/feat-video)

Video features

## Install

### Dependencies

A Python dependency is required for this plugin:

```bash
pip install youtube_transcript_api
```

Install the plugin:

```bash
npm i -g @agent-smith/feat-video
```

Add the plugin to your `config.yml` file and run the `conf` command to update the cli.

```yml
plugins:
  - "@agent-smith/feat-video"
```

```bash
lm conf ~/path/to/config.yml
```

**Warning**: the `youtube_transcript_api` does not work all the time. Some videos might get an error
while retrieving the transcript

## Jobs

### Chat with Youtube videos

To run an inference query vs a video transcript use the <kbd>ytv</kbd> job:

```bash
lm ytv "0VLAoVGf_74" "summarize this video"
```

The first argument is the Youtube video id and the second is the query. The
cli flags are available: example to chat with the video:

```bash
lm ytv "0VLAoVGf_74" "summarize this video" -c
```

### Models

Default model: `qwen2.5:3b-instruct-q8_0` with 32k context

Available <a href="javascript:openLink('/plugins/models')">models</a>: see
the `inference` models collection

To run the task with a 12b:

```bash
lm ytv "0VLAoVGf_74" "summarize this video" s=12b
```

To run the job using another arbitrary model use the `m` option and
specify the model and template to use like this:

```bash
lm ytv "0VLAoVGf_74" "summarize this video" -m qwen2.5:32b
```

## Actions

### Transcript a video to text

The <kbd>yt-transcript</kbd> action is available: to fetch a video transcript
and copy it to the clipboard:

```bash
lm yt-transcript "0VLAoVGf_74" --oc
```

## Tasks

### Query a video transcript

Get the default model: <kbd>llama3.2:3b-instruct-q8_0</kbd> (Ollama)

To run an inference query vs a video transcript use the <kbd>yt-chat</kbd> task. Example:
copy your transcript in the clipboard and run:

```bash
lm yt-chat instructions="summarize this video" --ic
```

To use different models see the `models` doc above.

#### Inference params

Default inference params for the samplers:

- `max_tokens`: 8192
- `top_k`: 0.0 (disabled)
- `top_p`: 1 (disabled)
- `min_p`: 0.05
- `temperature`: 0.6

To run a query using different inference params use the `ip` option:

```bash
lm yt-chat --instructions "summarize this video" --temperature 0.8 --min_p 0 --top_p 0.95 --top_k 40
```

